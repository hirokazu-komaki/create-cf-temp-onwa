#!/usr/bin/env python3
"""
Master Integration Test Script
CloudFormation Parameter Migration - Task 7.2 Implementation

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã‚¿ã‚¹ã‚¯7.2ã€Œçµ±åˆãƒ†ã‚¹ãƒˆã¨CI/CDæ¤œè¨¼ã€ã®å®Œå…¨ãªå®Ÿè£…ã§ã™ã€‚
GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®å®Œå…¨ãƒ†ã‚¹ãƒˆã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã®æ¤œè¨¼ã€
ç›£è¦–ã¨ãƒ­ã‚°è¨­å®šã®ç¢ºèªã‚’åŒ…æ‹¬çš„ã«å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import json
import subprocess
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import time
from datetime import datetime
import concurrent.futures
import threading

class MasterIntegrationTester:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.scripts_dir = self.project_root / "scripts"
        self.results_dir = self.project_root / "integration-test-results"
        self.results_dir.mkdir(exist_ok=True)
        
        # AWS ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        os.environ['AWS_PROFILE'] = 'mame-local-wani'
        os.environ['AWS_DEFAULT_REGION'] = 'ap-northeast-1'
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚åˆ»
        self.test_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        self.test_scripts = {
            'ci_cd_integration': {
                'script': 'ci-cd-integration-test.py',
                'description': 'CI/CDçµ±åˆãƒ†ã‚¹ãƒˆ',
                'timeout': 300
            },
            'github_actions_workflow': {
                'script': 'github-actions-workflow-validator.py',
                'description': 'GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ¤œè¨¼',
                'timeout': 180
            },
            'deployment_process': {
                'script': 'deployment-process-validator.py',
                'description': 'ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹æ¤œè¨¼',
                'timeout': 600
            },
            'monitoring_logging': {
                'script': 'monitoring-logging-validator.py',
                'description': 'ç›£è¦–ãƒ»ãƒ­ã‚°è¨­å®šæ¤œè¨¼',
                'timeout': 240
            },
            'phase3_final_validation': {
                'script': 'phase3-final-validation.py',
                'description': 'Phase 3æœ€çµ‚æ¤œè¨¼',
                'timeout': 300
            }
        }
        
        self.test_results = {}
        self.overall_success = True
        
    def log_info(self, message: str):
        """æƒ…å ±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] â„¹ï¸  {message}")
        
    def log_success(self, message: str):
        """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âœ… {message}")
        
    def log_warning(self, message: str):
        """è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âš ï¸  {message}")
        
    def log_error(self, message: str):
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âŒ {message}")

    def check_prerequisites(self) -> Tuple[bool, List[str]]:
        """å‰ææ¡ä»¶ã®ç¢ºèª"""
        self.log_info("å‰ææ¡ä»¶ã‚’ç¢ºèªä¸­...")
        
        issues = []
        
        # Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
        python_version = sys.version_info
        if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
            issues.append(f"Python 3.8ä»¥ä¸ŠãŒå¿…è¦ã§ã™ï¼ˆç¾åœ¨: {python_version.major}.{python_version.minor}ï¼‰")
        
        # å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç¢ºèª
        required_packages = ['boto3', 'yaml', 'jsonschema']
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                issues.append(f"å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {package}")
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å­˜åœ¨ç¢ºèª
        for test_name, test_config in self.test_scripts.items():
            script_path = self.scripts_dir / test_config['script']
            if not script_path.exists():
                issues.append(f"ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {script_path}")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        config_files = [
            "test-parameters-basic.json",
            "test-parameters-advanced.json",
            "test-parameters-enterprise.json"
        ]
        
        for config_file in config_files:
            config_path = self.project_root / config_file
            if not config_path.exists():
                issues.append(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        
        # CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å­˜åœ¨ç¢ºèª
        template_path = self.project_root / "cf-templates" / "compute" / "ec2" / "ec2-autoscaling.yaml"
        if not template_path.exists():
            issues.append(f"CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {template_path}")
        
        # GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å­˜åœ¨ç¢ºèª
        workflows_dir = self.project_root / ".github" / "workflows"
        if not workflows_dir.exists():
            issues.append(f"GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {workflows_dir}")
        else:
            workflow_files = list(workflows_dir.glob("*.yml")) + list(workflows_dir.glob("*.yaml"))
            active_workflows = [f for f in workflow_files if not f.name.endswith('.disabled')]
            if not active_workflows:
                issues.append("æœ‰åŠ¹ãªGitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        if issues:
            self.log_error("å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ã§å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            for issue in issues:
                self.log_error(f"  - {issue}")
            return False, issues
        else:
            self.log_success("ã™ã¹ã¦ã®å‰ææ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã™")
            return True, []

    def run_single_test(self, test_name: str, test_config: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        script_path = self.scripts_dir / test_config['script']
        description = test_config['description']
        timeout = test_config['timeout']
        
        self.log_info(f"å®Ÿè¡Œä¸­: {description}")
        
        start_time = time.time()
        
        try:
            # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=self.project_root
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # çµæœã®è§£æ
            success = result.returncode == 0
            
            test_result = {
                'test_name': test_name,
                'description': description,
                'success': success,
                'execution_time': execution_time,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'timestamp': datetime.now().isoformat()
            }
            
            if success:
                self.log_success(f"å®Œäº†: {description} ({execution_time:.1f}ç§’)")
            else:
                self.log_error(f"å¤±æ•—: {description} ({execution_time:.1f}ç§’)")
                self.log_error(f"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›: {result.stderr}")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            end_time = time.time()
            execution_time = end_time - start_time
            
            self.log_error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {description} ({timeout}ç§’)")
            
            return {
                'test_name': test_name,
                'description': description,
                'success': False,
                'execution_time': execution_time,
                'return_code': -1,
                'stdout': '',
                'stderr': f'ãƒ†ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout}ç§’ï¼‰',
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            
            self.log_error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {description} - {str(e)}")
            
            return {
                'test_name': test_name,
                'description': description,
                'success': False,
                'execution_time': execution_time,
                'return_code': -2,
                'stdout': '',
                'stderr': f'ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}',
                'timestamp': datetime.now().isoformat()
            }

    def run_tests_sequentially(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œ"""
        self.log_info("ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œä¸­...")
        
        for test_name, test_config in self.test_scripts.items():
            result = self.run_single_test(test_name, test_config)
            self.test_results[test_name] = result
            
            if not result['success']:
                self.overall_success = False
        
        return self.test_results

    def run_tests_parallel(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆç‹¬ç«‹æ€§ã®ã‚ã‚‹ãƒ†ã‚¹ãƒˆã®ã¿ï¼‰"""
        self.log_info("ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œä¸­...")
        
        # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªãƒ†ã‚¹ãƒˆï¼ˆAWS APIã‚’ä½¿ç”¨ã—ãªã„ã‚‚ã®ï¼‰
        parallel_tests = ['github_actions_workflow']
        
        # é †æ¬¡å®Ÿè¡ŒãŒå¿…è¦ãªãƒ†ã‚¹ãƒˆï¼ˆAWS APIã‚’ä½¿ç”¨ã™ã‚‹ã‚‚ã®ï¼‰
        sequential_tests = ['deployment_process', 'monitoring_logging', 'phase3_final_validation', 'ci_cd_integration']
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            parallel_futures = {}
            
            for test_name in parallel_tests:
                if test_name in self.test_scripts:
                    future = executor.submit(self.run_single_test, test_name, self.test_scripts[test_name])
                    parallel_futures[future] = test_name
            
            # ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã®çµæœã‚’åé›†
            for future in concurrent.futures.as_completed(parallel_futures):
                test_name = parallel_futures[future]
                result = future.result()
                self.test_results[test_name] = result
                
                if not result['success']:
                    self.overall_success = False
        
        # é †æ¬¡å®Ÿè¡Œ
        for test_name in sequential_tests:
            if test_name in self.test_scripts:
                result = self.run_single_test(test_name, self.test_scripts[test_name])
                self.test_results[test_name] = result
                
                if not result['success']:
                    self.overall_success = False
        
        return self.test_results

    def generate_summary_report(self) -> Dict[str, Any]:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result['success'])
        failed_tests = total_tests - passed_tests
        
        total_execution_time = sum(result['execution_time'] for result in self.test_results.values())
        
        summary = {
            'test_session': {
                'timestamp': self.test_timestamp,
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                'total_execution_time': total_execution_time,
                'overall_success': self.overall_success
            },
            'test_results': self.test_results,
            'environment_info': {
                'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                'platform': sys.platform,
                'working_directory': str(self.project_root)
            }
        }
        
        return summary

    def print_summary_report(self, summary: Dict[str, Any]):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"""
        session = summary['test_session']
        
        print("\n" + "=" * 100)
        print("ğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼ - Task 7.2: çµ±åˆãƒ†ã‚¹ãƒˆã¨CI/CDæ¤œè¨¼")
        print("=" * 100)
        
        print(f"å®Ÿè¡Œæ™‚åˆ»: {session['timestamp']}")
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {session['total_tests']}")
        print(f"æˆåŠŸ: {session['passed_tests']}")
        print(f"å¤±æ•—: {session['failed_tests']}")
        print(f"æˆåŠŸç‡: {session['success_rate']:.1f}%")
        print(f"ç·å®Ÿè¡Œæ™‚é–“: {session['total_execution_time']:.1f}ç§’")
        
        print("\nğŸ“‹ å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ:")
        print("-" * 80)
        
        for test_name, result in self.test_results.items():
            status = "âœ… PASS" if result['success'] else "âŒ FAIL"
            print(f"{result['description']}: {status} ({result['execution_time']:.1f}ç§’)")
            
            if not result['success']:
                # ã‚¨ãƒ©ãƒ¼ã®è¦ç´„ã‚’è¡¨ç¤º
                stderr_lines = result['stderr'].split('\n')
                error_summary = stderr_lines[0] if stderr_lines else "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
                print(f"  ã‚¨ãƒ©ãƒ¼: {error_summary}")
        
        print("\n" + "=" * 100)
        if self.overall_success:
            print("ğŸ‰ ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
            print("âœ… GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™")
            print("âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã¯é©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™")
            print("âœ… ç›£è¦–ã¨ãƒ­ã‚°è¨­å®šã¯æ­£å¸¸ã§ã™")
            print("âœ… Phase 3ã®æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹é€ ã¯å®Œå…¨ã«æ©Ÿèƒ½ã—ã¦ã„ã¾ã™")
            print("\nğŸš€ æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            print("âŒ ä¸€éƒ¨ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
            print("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯å€‹åˆ¥ã®ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("å•é¡Œã‚’è§£æ±ºã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
        print("=" * 100)

    def save_detailed_results(self, summary: Dict[str, Any]):
        """è©³ç´°çµæœã®ä¿å­˜"""
        # ãƒ¡ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
        summary_file = self.results_dir / f"integration-test-summary-{self.test_timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«
        for test_name, result in self.test_results.items():
            result_file = self.results_dir / f"{test_name}-result-{self.test_timestamp}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        
        # æœ€æ–°çµæœã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ï¼ˆWindowså¯¾å¿œï¼‰
        latest_summary = self.results_dir / "latest-integration-test-summary.json"
        try:
            if latest_summary.exists():
                latest_summary.unlink()
            # Windowsã§ã¯ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
            import shutil
            shutil.copy2(summary_file, latest_summary)
        except Exception:
            pass  # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
        
        self.log_success(f"è©³ç´°çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {summary_file}")
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆè¡¨ç¤º
        print(f"\nğŸ“„ ä¿å­˜ã•ã‚ŒãŸçµæœãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - ãƒ¡ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼: {summary_file}")
        for test_name in self.test_results.keys():
            result_file = self.results_dir / f"{test_name}-result-{self.test_timestamp}.json"
            print(f"  - {test_name}: {result_file}")

    def cleanup_old_results(self, keep_days: int = 7):
        """å¤ã„çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            current_time = time.time()
            cutoff_time = current_time - (keep_days * 24 * 60 * 60)
            
            cleaned_files = 0
            for result_file in self.results_dir.glob("*-result-*.json"):
                if result_file.stat().st_mtime < cutoff_time:
                    result_file.unlink()
                    cleaned_files += 1
            
            for summary_file in self.results_dir.glob("integration-test-summary-*.json"):
                if summary_file.stat().st_mtime < cutoff_time:
                    summary_file.unlink()
                    cleaned_files += 1
            
            if cleaned_files > 0:
                self.log_info(f"å¤ã„çµæœãƒ•ã‚¡ã‚¤ãƒ« {cleaned_files} å€‹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                
        except Exception as e:
            self.log_warning(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def run_comprehensive_integration_test(self, parallel: bool = False) -> bool:
        """åŒ…æ‹¬çš„ãªçµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("ğŸš€ Task 7.2: çµ±åˆãƒ†ã‚¹ãƒˆã¨CI/CDæ¤œè¨¼ - å®Ÿè¡Œé–‹å§‹")
        print("=" * 100)
        print("GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ")
        print("ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã®æ¤œè¨¼")
        print("ç›£è¦–ã¨ãƒ­ã‚°è¨­å®šã®ç¢ºèª")
        print("=" * 100)
        
        start_time = time.time()
        
        try:
            # å‰ææ¡ä»¶ã®ç¢ºèª
            prerequisites_ok, issues = self.check_prerequisites()
            if not prerequisites_ok:
                self.log_error("å‰ææ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ†ã‚¹ãƒˆã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
                return False
            
            # å¤ã„çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.cleanup_old_results()
            
            # ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            if parallel:
                self.run_tests_parallel()
            else:
                self.run_tests_sequentially()
            
            # çµæœã®ç”Ÿæˆã¨ä¿å­˜
            summary = self.generate_summary_report()
            self.save_detailed_results(summary)
            self.print_summary_report(summary)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"\nâ±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
            
            return self.overall_success
            
        except KeyboardInterrupt:
            self.log_error("ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            return False
        except Exception as e:
            self.log_error(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Task 7.2: çµ±åˆãƒ†ã‚¹ãƒˆã¨CI/CDæ¤œè¨¼')
    parser.add_argument('--parallel', action='store_true', 
                       help='å¯èƒ½ãªãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è©³ç´°ãªå‡ºåŠ›ã‚’è¡¨ç¤ºã™ã‚‹')
    
    args = parser.parse_args()
    
    tester = MasterIntegrationTester()
    
    try:
        success = tester.run_comprehensive_integration_test(parallel=args.parallel)
        
        if success:
            print("\nğŸ¯ Task 7.2 å®Œäº†: ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
            sys.exit(0)
        else:
            print("\nâŒ Task 7.2 å¤±æ•—: ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Task 7.2 å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()